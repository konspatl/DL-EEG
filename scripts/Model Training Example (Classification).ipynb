{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ebcb855",
   "metadata": {},
   "source": [
    "# Model Training for State-based EEG Decoding Tasks\n",
    "\n",
    "Author: Konstantinos Patlatzoglou\n",
    "\n",
    "A simple python pipeline for:\n",
    "1) Creating a deep learning model for EEG decoding (Tensorflow/Keras support)\n",
    "\n",
    "2) Training a model based on several selected EEG dataset and model parameters (multi-study integration support)\n",
    "\n",
    "\n",
    "## Packages required:\n",
    "* numpy \n",
    "* pathlib \n",
    "* natsort \n",
    "* pandas \n",
    "* h5py\n",
    "* scikit-learn\n",
    "* tensorflow>=2.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe8da9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy\n",
    "!pip install pathlib\n",
    "!pip install natsort\n",
    "!pip install pandas\n",
    "!pip install h5py\n",
    "!pip install scikit-learn\n",
    "!pip install tensorflow==2.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1f20c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "sys.path.append(str(Path.cwd().parent / 'DL-EEG')) # Add utils package\n",
    "\n",
    "from utils import utils\n",
    "from utils import EEG\n",
    "from utils import NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27842ab",
   "metadata": {},
   "source": [
    "# Model Training Parameters\n",
    "\n",
    "This pipeline has been developed to be compatible with the EEG dataset specifications found in *'export_dataset.py'*. \n",
    "\n",
    "Briefly, a number of EEG datasets can be selected based on the *EEG_DATASET* parameters and the *'EEG_DATASET_PARAMETERS.json'* specification file of the corresponding dataset directory (+ *subject_eeg_data.npy*, *subject_info.json*), which includes the available *subjects*, *states*, and *export* variables that can be incorporated as training/testing targets.\n",
    "\n",
    "Also, a number of model training parameters (*MODEL_PARAMETERS*) can be selected with respect to the model, the learning algorithm (*optimizer*, *learning rate*, *loss*, *metrics*, *batch size*, *epochs*) and several EEG-specific methods (*normalization* and *sample/target weighting*)\n",
    "\n",
    "**Dataset Directory:**\n",
    "\n",
    "*EEG_DATASET_PARAMETERS.json*:\n",
    "* *EEG_DATASET*:\n",
    " * *Study* - study name\n",
    " * *Subjects* - list of subjects\n",
    " * *States* - list of states\n",
    " * *Export* - list of export keys\n",
    " * *Other* - dict (Optional)\n",
    "* *EEG_PARAMETERS*:\n",
    " * *Channels* - name of channel selection\n",
    " * *Sfreq* - sampling frequency (Hz)\n",
    " * *Epoch Size* - epoch window size (sec)\n",
    " * *Reference* - reference montage name\n",
    " * *Topomap* - topomap representation (boolean) (Optional)\n",
    " * *...* (Optional)\n",
    "\n",
    "*subject_eeg_data.npy* - Epoched EEG Data (ndarray)\n",
    "\n",
    "*subject_info.json*:\n",
    " * *States* - list of states\n",
    " * *Channels* - list of channel names\n",
    " * *Export* - dict of export values per key and state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bd6778",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_DIR_NAME = 'Model Training (Classification)'  # Name of Model Training Export Directory\n",
    "\n",
    "\n",
    "NO_OF_DATASETS = 1\n",
    "\n",
    "EEG_DATASET_1 = {'Study': 'Cambridge Anesthesia',\n",
    "                 'Subjects': ['S1', 'S2'],\n",
    "                 'States': ['Wakefulness', 'Sedation'],\n",
    "                 'Targets': '1-hot', # '1-hot' (Classifcation), list [target1, target2, ] (Regression)\n",
    "                 'Target Values': [[1,0], [0,1]], # None, 'Read_from_info_file', list [[], [], ...]\n",
    "                 }\n",
    "\n",
    "CLASSES = ['Wakefulness', 'Sedation']  # list (Classification) or None (Regression)\n",
    "\n",
    "MODEL_PARAMETERS = {'Model': 'EEGNet_v2', # 'Toy', 'cNN_3D', 'cNN_topomap' 'BD_Deep4', 'BD_Shallow', 'EEGNet_v1', 'EEGNet_v2'\n",
    "                    'EEG Normalization': 'epoch_stand_robust',  # 'epoch_stand', 'epoch_stand_robust', 'epoch_l2_norm'\n",
    "                    'Sample Weights': True, #\n",
    "                    'Target Weights': True,\n",
    "                    'Shuffle Samples': True,\n",
    "                    'Optimizer': 'Adadelta', # tensorflow  parameter ('Adadelta', 'Adam', 'SGD')\n",
    "                    'Learning Rate': 1,\n",
    "                    'Loss': 'categorical_crossentropy', # tensorflow parameter (e.g. 'categorical_crossentropy', 'mean_squared_error')\n",
    "                    'Metrics': ['accuracy'],  # tensorflow parameter (e.g. 'accuracy', 'mean_absolute_error')\n",
    "                    'Batch Size': 64,  # samples per gradient update\n",
    "                    'Epochs': 10  # no of iterations over the training dataset\n",
    "                    }\n",
    "\n",
    "MODEL_CHECKPOINT = True  # If True, Save Model in Each Training Epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a3ea7c",
   "metadata": {},
   "source": [
    "# Import EEG Datasets\n",
    "\n",
    "First, we import the selected EEG datasets and concatenate them into a single *eeg_dataset* dictionary. Datasets are integrated after checking consistency across the *EEG sampling frequency*, *channel names*, *reference montage*, *epoch size* and *targets*. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221b6d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_path = Path.cwd()\n",
    "\n",
    "datasets_path = current_path.parent / 'data'\n",
    "\n",
    "eeg_datasets = []\n",
    "for i in range(NO_OF_DATASETS): # For each specified Dataset\n",
    "\n",
    "    EEG_DATASET = globals()['EEG_DATASET_' + str(i+1)]\n",
    "    dataset_path = EEG.find_dataset_path(datasets_path, EEG_DATASET)\n",
    "\n",
    "    if dataset_path is None:\n",
    "        print('No dataset with the given parameters found')\n",
    "        exit(1)\n",
    "\n",
    "    # Get the Dataset from the corresponding directory\n",
    "    # Dict ('EEG Dataset Parameters', 'Subjects', 'EEG Data', 'Info')\n",
    "    raw_eeg_dataset = EEG.get_dataset(dataset_path)\n",
    "\n",
    "    # Select the Data specified in EEG_DATASET with respect to Subjects, States and Targets\n",
    "    # Dict ('EEG Dataset Parameters', 'Subjects', 'Dataset ID', 'EEG Data', 'States', 'Channels', 'Targets',\n",
    "    #       'Target Values')\n",
    "    eeg_dataset = EEG.select_data_from_dataset(raw_eeg_dataset, EEG_DATASET, id=i)\n",
    "    eeg_datasets.append(eeg_dataset)\n",
    "\n",
    "# Check Datasets Consistency (Sampling Frequency, Reference Montage, Channel names, Epoch Size, Targets)\n",
    "if not EEG.check_datasets_consistency(eeg_datasets):\n",
    "    print('Datasets cannot be integrated')\n",
    "    exit(2)\n",
    "\n",
    "# Concatenate All Datasets\n",
    "eeg_dataset = EEG.concatenate_datasets(eeg_datasets)\n",
    "del eeg_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3b3bca",
   "metadata": {},
   "source": [
    "*eeg_dataset*:\n",
    "* *eeg_dataset_parameters* (list): List of EEG_DATASET_PARAMETERS for each dataset\n",
    "* *subjects* (list): list of subjects (subject,)\n",
    "* *dataset_id* (list): list of dataset ids (subject,)\n",
    "* *eeg_data_list* (list): list of eeg data (subject,) (state,) (epoch, channel, sample)\n",
    "* *states_list* (list): list of states (subject,) (state,)\n",
    "* *channels_list* (list): list of channel names (subject,)\n",
    "* *targets* (str or list): target name or list of target names\n",
    "* *targets_list* (list or None): list of target values (subject,) (state,) (epoch, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e4e83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_dataset_parameters = eeg_dataset['EEG Dataset Parameters']\n",
    "subjects = eeg_dataset['Subjects']\n",
    "dataset_id = eeg_dataset['Dataset ID']\n",
    "eeg_data_list = eeg_dataset['EEG Data']\n",
    "states_list = eeg_dataset['States']\n",
    "channels_list = eeg_dataset['Channels']\n",
    "targets = eeg_dataset['Targets']\n",
    "targets_list = eeg_dataset['Target Values']\n",
    "\n",
    "del eeg_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d2e8aa",
   "metadata": {},
   "source": [
    "# EEG Pre-Processing\n",
    "\n",
    "The EEG data need to be pre-processed before we feed them into the deep learning networks. This process includes the *normalization* of the epoch instances (as EEG data are typically in Î¼V) and *reshaping* the dimensions, which is a requirement for DL-based EEG models (e.g. with respect to channel information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5030cf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Topomap Model Compatibility\n",
    "if MODEL_PARAMETERS['Model'] == 'cNN_topomap':\n",
    "    if not EEG.check_EEG_model_compatibility(eeg_data_list, 'cNN_topomap'):\n",
    "        print('EEG data shape is incompatible with selected Model')\n",
    "        exit(3)\n",
    "\n",
    "# Perform EEG Normalization (e.g. epoch-wise Standardization)\n",
    "eeg_data_list = EEG.normalize_EEG_data_list(eeg_data_list, MODEL_PARAMETERS['EEG Normalization'])\n",
    "\n",
    "# Reshape EEG data according to Model + Add Kernel Dimension\n",
    "# e.g. If Toy Model (1D), reshape EEG data into (epoch, sample)\n",
    "# e.g. If cNN_3D model (3D), reshape EEG data into (epoch, channel, channel, sample, 1)\n",
    "eeg_data_list = EEG.reshape_EEG_data_list(eeg_data_list, channels_list, MODEL_PARAMETERS['Model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b23885",
   "metadata": {},
   "source": [
    "# Model Selection\n",
    "\n",
    "We select and create a deep learning model specified within the NN module. Model configuration includes the *input shape* of the EEG data, the *output shape* of the target vectors, the *sampling frequency* of EEG, and the selected *learning algorithm* (classification/regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502db238",
   "metadata": {},
   "outputs": [],
   "source": [
    "if targets_list is None:\n",
    "    print('Cannot perform Model Training. Missing Targets')\n",
    "    exit(4)\n",
    "\n",
    "# Model Info\n",
    "input_shape = eeg_data_list[0][0].shape[1:]  # Model Input Shape\n",
    "output_shape = targets_list[0][0].shape[1:]  # Model Output Shape\n",
    "sfreq = eeg_dataset_parameters[0]['EEG_PARAMETERS']['Sfreq']  # EEG Sampling Frequency\n",
    "\n",
    "if targets == '1-hot':  # Classification\n",
    "    classification = True\n",
    "else:  # Regression\n",
    "    classification = False\n",
    "\n",
    "# Create Model\n",
    "model = NN.create_model(MODEL_PARAMETERS['Model'], input_shape, sfreq, output_shape, classification=classification)\n",
    "model.summary()\n",
    "\n",
    "model_memory_usage = NN.get_model_memory_usage(model, MODEL_PARAMETERS['Batch Size'])\n",
    "print('Model Memory Usage:' + str(model_memory_usage) + ' GB')\n",
    "print()\n",
    "\n",
    "# Compile Model\n",
    "optimizer = NN.get_optimizer(MODEL_PARAMETERS['Optimizer'], learning_rate=MODEL_PARAMETERS['Learning Rate'])\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=MODEL_PARAMETERS['Loss'],\n",
    "              metrics=MODEL_PARAMETERS['Metrics'])\n",
    "\n",
    "model_info = {'Input shape': input_shape,\n",
    "              'Sfreq': sfreq,\n",
    "              'Output shape': output_shape,\n",
    "              'Targets': targets\n",
    "              }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aae8fdc",
   "metadata": {},
   "source": [
    "# Create Results Directory\n",
    "\n",
    "A results directory is created to store the trained model, along with the specified training parameters (*EEG_DATASET_PARAMETERS*, *MODEL_PARAMETERS*, *MODEL_INFO*, *CLASSES* (Optional))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3073ccff",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_path = current_path.parent / 'results' / RESULTS_DIR_NAME\n",
    "utils.create_directory(result_path)\n",
    "\n",
    "# For each Dataset, save EEG_DATASET_PARAMETERS (EEG_DATASET + EEG_PARAMETERS)\n",
    "for i in range(NO_OF_DATASETS):\n",
    "\n",
    "    EEG_DATASET = globals()['EEG_DATASET_' + str(i + 1)]\n",
    "    if 'Other' in eeg_dataset_parameters[i]['EEG_DATASET']:  # Concatenate 'Other' in EEG_DATASET\n",
    "        EEG_DATASET['Other'] = eeg_dataset_parameters[i]['EEG_DATASET']['Other']\n",
    "    EEG_PARAMETERS = eeg_dataset_parameters[i]['EEG_PARAMETERS']\n",
    "\n",
    "    EEG_DATASET_PARAMETERS = {'EEG_DATASET': EEG_DATASET, 'EEG_PARAMETERS':EEG_PARAMETERS}\n",
    "    json.dump(EEG_DATASET_PARAMETERS, open(str(result_path / ('EEG_DATASET_PARAMETERS_' + str(i + 1) + '.json')),\n",
    "                                           'w'), indent=4)\n",
    "\n",
    "# Save MODEL_PARAMETERS\n",
    "json.dump(MODEL_PARAMETERS, open(str(result_path / 'MODEL_PARAMETERS.json'), 'w'), indent=4)\n",
    "\n",
    "# Save MODEL_INFO\n",
    "json.dump(model_info, open(str(result_path / 'MODEL_INFO.json'), 'w'), indent=4)\n",
    "\n",
    "# If Classification, Save CLASSSES\n",
    "if model_info['Targets'] == '1-hot':\n",
    "    json.dump(CLASSES, open(str(result_path / 'CLASSES.json'), 'w'), indent=4)\n",
    "\n",
    "checkpoint_path = None\n",
    "if MODEL_CHECKPOINT:\n",
    "    checkpoint_path = result_path / 'Model Checkpoint'\n",
    "    utils.create_directory(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7284c3",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "Before we fit our EEG data to the model, we concatenate all subjects and states into our input and output tensors (*Xtr_t*, *Ytr_t*).\n",
    "\n",
    "Optionally, we can calculate a sample/target weighting vector for our training instances, based on a number of normalization factors. This is important when training models with unbalanced or heterogeneous datasets, in terms of state lengths, number of subjects and other dataset categorical differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e600aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Subjects' + str(subjects))\n",
    "\n",
    "# Sample Weights\n",
    "sample_weight_list = None\n",
    "if MODEL_PARAMETERS['Sample Weights']:\n",
    "\n",
    "    dataset_categories = None\n",
    "    if all(['Other' in eeg_dataset['EEG_DATASET'] for eeg_dataset in eeg_dataset_parameters]):\n",
    "        dataset_categories = [eeg_dataset['EEG_DATASET']['Other']['Drug']\n",
    "                              for eeg_dataset in eeg_dataset_parameters]\n",
    "    sample_weight_list = EEG.get_sample_weights_list(eeg_data_list, dataset_id=dataset_id,\n",
    "                                                           dataset_categories=dataset_categories)\n",
    "\n",
    "# Concatenate Subjects and States\n",
    "Xtr, Ytr, Xtr_w = utils.concatenate_data(eeg_data_list, targets_list, sample_weights_list=sample_weight_list)\n",
    "\n",
    "# Target Weights\n",
    "if MODEL_PARAMETERS['Target Weights']:\n",
    "    if MODEL_PARAMETERS['Sample Weights']:\n",
    "        Xtr_w = Xtr_w * utils.get_target_weights(Ytr)\n",
    "        Xtr_w *= len(Xtr_w) / np.sum(Xtr_w)\n",
    "    else:\n",
    "        Xtr_w = utils.get_target_weights(Ytr)\n",
    "\n",
    "# Shuffle Train Data\n",
    "if MODEL_PARAMETERS['Shuffle Samples']:\n",
    "    if Xtr_w is None:\n",
    "        Xtr, Ytr = shuffle(Xtr, Ytr)\n",
    "    else:\n",
    "        Xtr, Ytr, Xtr_w = shuffle(Xtr, Ytr, Xtr_w)\n",
    "\n",
    "del eeg_data_list, targets_list, sample_weight_list\n",
    "\n",
    "# ------------------------- Model Fitting -----------------------------------\n",
    "print('Number of training instances: ' + str(Xtr.shape[0]))\n",
    "\n",
    "# Training Data\n",
    "Xtr_t, Ytr_t = utils.get_tensor_maps(Xtr, Y=Ytr, classification=classification)\n",
    "\n",
    "callbacks = None\n",
    "if MODEL_CHECKPOINT: # If True, Save Model in Each Training Epoch\n",
    "    filename = checkpoint_path / 'Model_{epoch:02d}.tf'\n",
    "    checkpoint = NN.get_ModelCheckpoint(str(filename), save_freq='epoch')\n",
    "    callbacks = [checkpoint]\n",
    "\n",
    "history = model.fit(Xtr_t, Ytr_t,\n",
    "                    batch_size=MODEL_PARAMETERS['Batch Size'],\n",
    "                    epochs=MODEL_PARAMETERS['Epochs'],\n",
    "                    verbose=2,\n",
    "                    sample_weight=Xtr_w,\n",
    "                    callbacks=callbacks)\n",
    "\n",
    "history = history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c5509c",
   "metadata": {},
   "source": [
    "# Model Export\n",
    "\n",
    "Finally, we save the trained model (+ history) in our resutls directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c4dff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(str(result_path / 'Model.tf'), include_optimizer=True, save_format='tf')\n",
    "\n",
    "if MODEL_CHECKPOINT:\n",
    "    # Export Training History as a json file\n",
    "    json.dump(history, open(str(checkpoint_path / 'training_history.json'), 'w'), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8091f7a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
